{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0113c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역 변수 설정\n",
    "DATA_SIZE=1000\n",
    "VOCAB_SIZE = 10000  # 단어 사전 크기\n",
    "MAX_LEN = 200       # 패딩할 최대 문장 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5780d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     1000 non-null   object\n",
      " 1   sentiment  1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "                                              review sentiment\n",
      "0  I really liked this Summerslam due to the look...  positive\n",
      "1  Not many television shows appeal to quite as m...  positive\n",
      "2  The film quickly gets to a major chase scene w...  negative\n",
      "3  Jane Austen would definitely approve of this o...  positive\n",
      "4  Expectations were somewhat high for me when I ...  negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/imdb.csv\") # https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "df = df.sample(n=DATA_SIZE, random_state=42).reset_index() # 랜덤 1000개 추출\n",
    "df = df.drop(\"index\", axis=1)\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03b1902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10\n"
     ]
    }
   ],
   "source": [
    "# HTML 태그 제거\n",
    "import re\n",
    "import html # HTML 엔티티 처리를 위해 import\n",
    "\n",
    "def remove_html_tags_regex(text):\n",
    "    # 1. HTML 태그 제거: <로 시작해서 >로 끝나는 모든 것을 찾음\n",
    "    # <.*?> : . (모든 문자)가 * (0번 이상) 반복되는데, ? (non-greedy)\n",
    "    # ?가 없으면 \"<b>text</b>\" 전체를 태그로 인식할 수 있음\n",
    "    pattern = re.compile('<.*?>')\n",
    "    cleaned_text = re.sub(pattern, ' ', text) # 태그를 공백(' ')으로 치환\n",
    "    \n",
    "    # 2. HTML 엔티티 변환 (e.g., &nbsp; -> 공백, &lt; -> <)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    \n",
    "    # 3. 여러 개의 공백을 하나로 합침\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df['review'] = df['review'].apply(remove_html_tags_regex)\n",
    "print(df[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14ee9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 시작...\n",
      "전처리 완료.\n",
      "\n",
      "--- 전처리 후 샘플 ---\n",
      "really liked summerslam due look arena curtains look overall interesting reason anyways could one best summerslam's ever wwf lex luger main event yokozuna time ok huge fat man vs strong man glad times changed terrible main event like every match luger terrible matches card razor ramon vs ted dibiase steiner brothers vs heavenly bodies shawn michaels vs curt hening event shawn named big monster body guard diesel irs vs kid bret hart first takes doink takes jerry lawler stuff harts lawler always interesting ludvig borga destroyed marty jannetty undertaker took giant gonzalez another terrible match smoking gunns tatanka took bam bam bigelow headshrinkers yokozuna defended world title lex luger match boring terrible ending however deserves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cmjcm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 영어 불용어 리스트 로드\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 텍스트 정제 및 불용어 제거를 한 번에 처리하는 함수\n",
    "def clean_and_remove_stopwords(text):\n",
    "    \n",
    "    # 소문자 변환 및 정제 (알파벳, 공백, '만 남김)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\']', '', text) \n",
    "    \n",
    "    # 토큰화 (띄어쓰기 기준)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 불용어 제거\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 다시 하나의 문자열로 합치기\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "print(\"전처리 시작...\")\n",
    "df['review'] = df['review'].apply(clean_and_remove_stopwords)\n",
    "\n",
    "print(\"전처리 완료.\")\n",
    "print(\"\\n--- 전처리 후 샘플 ---\")\n",
    "print(df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d42e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextVectorization 레이어 어휘 학습(adapt) 시작...\n",
      "어휘 학습 완료.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# 1. TextVectorization 레이어 초기화\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation', # 1. 소문자 변환, 2. 구두점 제거\n",
    "    split='whitespace',                        # 3. 띄어쓰기 기준 토큰화\n",
    "    max_tokens=VOCAB_SIZE,                     # 4. 단어 사전 크기 (OOK 토큰 포함)\n",
    "    output_mode='int',                         # 5. 정수 인코딩\n",
    "    output_sequence_length=MAX_LEN             # 6. 패딩 (길이 통일)\n",
    ")\n",
    "\n",
    "# 2. 전처리된 텍스트 데이터로 단어 사전(vocabulary) 생성\n",
    "print(\"TextVectorization 레이어 어휘 학습(adapt) 시작...\")\n",
    "vectorize_layer.adapt(df['review'])\n",
    "print(\"어휘 학습 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0102d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1] 원본 텍스트 샘플 ---\n",
      "really liked summerslam due look arena curtains look overall interesting reason anyways could one best summerslam's ever wwf lex luger main event yokozuna time ok huge fat man vs strong man glad times changed terrible main event like every match luger terrible matches card razor ramon vs ted dibiase steiner brothers vs heavenly bodies shawn michaels vs curt hening event shawn named big monster body guard diesel irs vs kid bret hart first takes doink takes jerry lawler stuff harts lawler always interesting ludvig borga destroyed marty jannetty undertaker took giant gonzalez another terrible match smoking gunns tatanka took bam bam bigelow headshrinkers yokozuna defended world title lex luger match boring terrible ending however deserves\n",
      "many television shows appeal quite many different kinds fans like farscape doesi know youngsters years oldfans male female many different countries think adore tv miniseries elements found almost every show tv character driven drama could australian soap opera yet episode science fact fiction would give even hardiest trekkie run money brainbender stakes wormhole theory time travel true equational formmagnificent embraces cultures map possibilities endless multiple stars therefore thousands planets choose broad scope would expected nothing would able keep illusion long farscape really comes elementit succeeds others failed especially likes star trek universe practically zero kaos element ran ideas pretty quickly kept rehashing course seasons manage keep audience's attention using good continuity constant character evolution multiple threads every episode unique personal touches camera specific certain character groups within whole structure allows extremely large area subject matter loyalties forged broken many ways many many issues happened see pilot premiere passing keep tuning see crichton would ever get girl seeing television delighted see available dvd admit thing kept sane whilst hour night shift developed chronic insomniafarscape thing get extremely long nights favour watch pilot see mean farscape comet\n",
      "film quickly gets major chase scene ever increasing destruction first really bad thing guy hijacking steven seagal would beaten pulp seagal's driving probably would ended whole premise movie seems like decided make kinds changes movie plot plan enjoy action expect coherent plot turn sense logic may reduce chance getting headache give hope steven seagal trying move back towards type characters portrayed popular movies\n",
      "\n",
      "--- [2] Vectorize 결과 (정수 인코딩 + 패딩) ---\n",
      "tf.Tensor(\n",
      "[[  10  254 6636  312   68    1    1   68  249   85  157 3862   31    4\n",
      "    33    1   42 9253 5499 5474  179 1183 4782    9  357  801 3121   44\n",
      "  1540  449   44  802  115 1059  211  179 1183    5   77  989 5474  211\n",
      "  2339 3218 7092    1 1540 4925    1 6687  749 1540    1 1653 4035 5421\n",
      "  1540    1    1 1183 4035  738   83  559  462 4402    1    1 1540  391\n",
      "     1 7987   18  212    1  212 2115 7703  298    1 7703  137   85    1\n",
      "     1 2159 4266    1 4859  514 1311    1   57  211  989 2265    1    1\n",
      "   514 8964 8964 8910    1 4782    1  107  284 5499 5474  989  248  211\n",
      "   175   90  697    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  23  448  176 1264   86   23  161 2112  255    5 5776    1   36 6280\n",
      "    58    1 1164  629   23  161 3766   29 6257  114  943  771  141  135\n",
      "    77   30  114   39 1631  380   31 1989 1024 1092  113  289  847   76\n",
      "   865   11   78    8    1    1  432  163    1    1 3894 3336    9 1275\n",
      "   198    1    1    1 4565 3023 2308 2406 3542  355 1071 1549    1 1194\n",
      "  3825 2925   11  567   64   11  264  309 5616  145 5776   10  185    1\n",
      "  1685  223  830  149  946  322 1827 2230 2574 2004    1  958 1575 1044\n",
      "    97  935  625    1  153 4049 1733  309  874  702  538    6 2169 1511\n",
      "    39 2697 3542 3964   77  289  814  599 1444  235 2898  592   39 2668\n",
      "   488  108 1836 1431  366  948 1992  845  339    1    1 1794   23  753\n",
      "    23   23  898  378    7 4161 5245 3502  309    1    7    1   11   42\n",
      "    13  143  243  448 8501    7 1261  158  727   55  625 4057 1817  411\n",
      "   174 5055 1246 8729    1   55   13  366  145 1877 8223   24 4161    7\n",
      "   300 5776    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   3  935  103  645 1422   65   42    1 5901   18   10   14   55   75\n",
      "     1 1690 2536   11 3840 2563    1 1630  119   11  718  108  598    2\n",
      "    79    5  552   25 2112 1976    2   46 1155  208  118  551 6024   46\n",
      "   320  170 1735   95 7070  413  216    1   78  245 1690 2536  138  604\n",
      "    53  557  325   27  792  761   28    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]], shape=(3, 200), dtype=int64)\n",
      "\n",
      "--- [3] Numpy 배열로 변환 ---\n",
      "[[  10  254 6636  312   68    1    1   68  249   85  157 3862   31    4\n",
      "    33    1   42 9253 5499 5474  179 1183 4782    9  357  801 3121   44\n",
      "  1540  449   44  802  115 1059  211  179 1183    5   77  989 5474  211\n",
      "  2339 3218 7092    1 1540 4925    1 6687  749 1540    1 1653 4035 5421\n",
      "  1540    1    1 1183 4035  738   83  559  462 4402    1    1 1540  391\n",
      "     1 7987   18  212    1  212 2115 7703  298    1 7703  137   85    1\n",
      "     1 2159 4266    1 4859  514 1311    1   57  211  989 2265    1    1\n",
      "   514 8964 8964 8910    1 4782    1  107  284 5499 5474  989  248  211\n",
      "   175   90  697    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  23  448  176 1264   86   23  161 2112  255    5 5776    1   36 6280\n",
      "    58    1 1164  629   23  161 3766   29 6257  114  943  771  141  135\n",
      "    77   30  114   39 1631  380   31 1989 1024 1092  113  289  847   76\n",
      "   865   11   78    8    1    1  432  163    1    1 3894 3336    9 1275\n",
      "   198    1    1    1 4565 3023 2308 2406 3542  355 1071 1549    1 1194\n",
      "  3825 2925   11  567   64   11  264  309 5616  145 5776   10  185    1\n",
      "  1685  223  830  149  946  322 1827 2230 2574 2004    1  958 1575 1044\n",
      "    97  935  625    1  153 4049 1733  309  874  702  538    6 2169 1511\n",
      "    39 2697 3542 3964   77  289  814  599 1444  235 2898  592   39 2668\n",
      "   488  108 1836 1431  366  948 1992  845  339    1    1 1794   23  753\n",
      "    23   23  898  378    7 4161 5245 3502  309    1    7    1   11   42\n",
      "    13  143  243  448 8501    7 1261  158  727   55  625 4057 1817  411\n",
      "   174 5055 1246 8729    1   55   13  366  145 1877 8223   24 4161    7\n",
      "   300 5776    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   3  935  103  645 1422   65   42    1 5901   18   10   14   55   75\n",
      "     1 1690 2536   11 3840 2563    1 1630  119   11  718  108  598    2\n",
      "    79    5  552   25 2112 1976    2   46 1155  208  118  551 6024   46\n",
      "   320  170 1735   95 7070  413  216    1   78  245 1690 2536  138  604\n",
      "    53  557  325   27  792  761   28    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "\n",
      "--- [4] 첫 번째 문장 결과 (앞 20개, 뒤 20개) ---\n",
      "앞부분: [  10  254 6636  312   68    1    1   68  249   85  157 3862   31    4\n",
      "   33    1   42 9253 5499 5474]\n",
      "뒷부분: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인하기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. 원본 텍스트 샘플 준비 (수동 전처리 완료된 상태)\n",
    "sample_texts = df['review'].head(3).tolist()\n",
    "print(\"--- [1] 원본 텍스트 샘플 ---\")\n",
    "for text in sample_texts:\n",
    "    print(text)\n",
    "\n",
    "# 2. vectorize_layer에 샘플 텍스트 통과\n",
    "# (레이어 자체를 함수처럼 사용)\n",
    "vectorized_output = vectorize_layer(sample_texts)\n",
    "\n",
    "print(\"\\n--- [2] Vectorize 결과 (정수 인코딩 + 패딩) ---\")\n",
    "print(vectorized_output)\n",
    "\n",
    "# 3. numpy 배열로 변환하면 더 보기 편함\n",
    "print(\"\\n--- [3] Numpy 배열로 변환 ---\")\n",
    "print(vectorized_output.numpy())\n",
    "\n",
    "# 4. 첫 번째 문장의 결과 확인 (MAX_LEN=200 가정)\n",
    "print(\"\\n--- [4] 첫 번째 문장 결과 (앞 20개, 뒤 20개) ---\")\n",
    "first_result = vectorized_output.numpy()[0]\n",
    "print(\"앞부분:\", first_result[:20])\n",
    "print(\"뒷부분:\", first_result[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1b4b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 총 어휘 사전 크기 (VOCAB_SIZE) ---\n",
      "10000\n",
      "\n",
      "--- [1] 앞부분 단어 20개 ---\n",
      "['', '[UNK]', 'movie', 'film', 'one', 'like', 'good', 'see', 'even', 'time', 'really', 'would', 'story', 'get', 'bad', 'much', 'people', 'well', 'first', 'great']\n",
      "\n",
      "--- [2] 뒷부분 단어 20개 (빈도가 낮은 단어들) ---\n",
      "['udo', 'ubisoft', 'uberrare', 'ubercoldness', 'tyrone', 'tyre', 'tyrant', 'typos', 'typo', 'typescooper', 'typecasting', 'tylos', 'tyloat', 'tykwer', 'tykes', 'tying', 'tyd', 'twos', 'tworeel', 'twoparter']\n",
      "\n",
      "--- [3] 'movie'라는 단어의 인덱스(정수) 찾기 ---\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 레이어의 어휘 사전(단어 목록) 가져오기\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "print(f\"--- 총 어휘 사전 크기 (VOCAB_SIZE) ---\")\n",
    "print(len(vocab))\n",
    "\n",
    "print(\"\\n--- [1] 앞부분 단어 20개 ---\")\n",
    "# 0번: 패딩(PAD) 토큰 (표시는 안 될 수 있음)\n",
    "# 1번: OOV(Out-of-Vocabulary) 토큰 ([UNK])\n",
    "print(vocab[:20])\n",
    "\n",
    "print(\"\\n--- [2] 뒷부분 단어 20개 (빈도가 낮은 단어들) ---\")\n",
    "print(vocab[-20:])\n",
    "\n",
    "print(\"\\n--- [3] 'movie'라는 단어의 인덱스(정수) 찾기 ---\")\n",
    "try:\n",
    "    print(vocab.index('movie'))\n",
    "except ValueError:\n",
    "    print(\"'movie'는 어휘 사전에 없습니다. (또는 VOCAB_SIZE 밖에 있음)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5973023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 2. 레이어를 내보내기(export) 위한 더미 모델 생성\n",
    "# TextVectorization 레이어만 포함하는 모델을 만듭니다.\n",
    "model_for_export = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer\n",
    "])\n",
    "\n",
    "# 3. 모델 저장 (이 폴더를 압축해서 전달)\n",
    "model_for_export.save('model/vectorizer_layer_model.keras')\n",
    "\n",
    "# load_model('model/vectorizer_layer_model') 을 통해 불러올 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-team",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
