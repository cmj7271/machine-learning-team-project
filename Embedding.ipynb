{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec89fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "데이터 로드 및 결측치 제거\n",
      "================================================================================\n",
      "\n",
      "데이터 로딩 중...\n",
      "GoEmotions 데이터: (57160, 30)\n",
      "Reddit 데이터: (36941, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"데이터 로드 및 결측치 제거\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 1. 데이터 불러오기\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n데이터 로딩 중...\")\n",
    "df_goemotions = pd.read_csv('data/clean_goemotions.csv')\n",
    "df_reddit = pd.read_csv('data/clean_reddit.csv')\n",
    "\n",
    "# 결측치(빈 텍스트) 제거 (저장/로드 과정에서 생길 수 있는 NaN 제거)\n",
    "df_goemotions.dropna(subset=['text'], inplace=True)\n",
    "df_reddit.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "print(f\"GoEmotions 데이터: {df_goemotions.shape}\")\n",
    "print(f\"Reddit 데이터: {df_reddit.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a39c314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tokenizer 실행 (텍스트 -> 정수 인덱스)\n",
      "================================================================================\n",
      "\n",
      "전체 단어 집합 크기: 41,690개\n",
      "\n",
      "[변환 예시]\n",
      "원문: that game hurt\n",
      "정수 인덱스: [9, 168, 659]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Tokenizer 실행 (텍스트 -> 정수 인덱스)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Tokenizer 실행 (텍스트 -> 정수 인덱스)\n",
    "# ==========================================\n",
    "# 두 데이터를 합쳐서 전체 단어 사전을 만듭니다 (Reddit의 신조어도 포함하기 위해)\n",
    "all_texts = list(df_goemotions['text'].astype(str)) + list(df_reddit['body'].astype(str))\n",
    "\n",
    "# Tokenizer 객체 생성 (<OOV>는 사전에 없는 단어 처리용)\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# 단어 집합 크기 (Vocab Size) + 1 (패딩용 0 때문에 +1)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"\\n전체 단어 집합 크기: {vocab_size:,}개\")\n",
    "\n",
    "# 텍스트를 정수 시퀀스로 변환\n",
    "goemotions_sequences = tokenizer.texts_to_sequences(df_goemotions['text'].astype(str))\n",
    "reddit_sequences = tokenizer.texts_to_sequences(df_reddit['body'].astype(str))\n",
    "\n",
    "# 변환 예시 확인\n",
    "print(f\"\\n[변환 예시]\")\n",
    "print(f\"원문: {df_goemotions['text'].iloc[0]}\")\n",
    "print(f\"정수 인덱스: {goemotions_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f614b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "패딩 (Padding) - 길이 맞추기\n",
      "================================================================================\n",
      "\n",
      "[패딩 결과]\n",
      "GoEmotions 입력 데이터 Shape: (57160, 40)\n",
      "Reddit 입력 데이터 Shape: (36941, 40)\n",
      "\n",
      "패딩된 데이터 예시: [  9 168 659   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"패딩 (Padding) - 길이 맞추기\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 패딩 (Padding) - 길이 맞추기\n",
    "# ==========================================\n",
    "# 문장 길이를 맞춰줍니다. (40으로 설정)\n",
    "max_length = 40\n",
    "padding_type = 'post'   # 문장 뒤에 0을 채움 (0, 0, 0...)\n",
    "trunc_type = 'post'     # 문장이 길면 뒤를 자름\n",
    "\n",
    "goemotions_padded = pad_sequences(goemotions_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "reddit_padded = pad_sequences(reddit_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(f\"\\n[패딩 결과]\")\n",
    "print(f\"GoEmotions 입력 데이터 Shape: {goemotions_padded.shape}\")\n",
    "print(f\"Reddit 입력 데이터 Shape: {reddit_padded.shape}\")\n",
    "print(f\"\\n패딩된 데이터 예시: {goemotions_padded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45b73e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "임베딩(Word2Vec) 변환\n",
      "================================================================================\n",
      "\n",
      "--- [변환 결과 확인] ---\n",
      "입력 문장(정수 인덱스): [  9 168 659   0   0   0   0   0   0   0] ... (길이: 40)\n",
      "임베딩 후 벡터 형태(Shape): (40, 64) (문장길이 40 x 벡터차원 64)\n",
      "첫 번째 단어의 벡터값(일부): [-0.03509829  0.00182395 -0.03594821 -0.0102967  -0.03926591] ...\n",
      "\n",
      "------------------------\n",
      "※ 현재 벡터값은 랜덤 초기화 상태입니다. 모델 학습(fit)을 돌리면 의미 있는 값으로 업데이트됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"임베딩(Word2Vec) 변환\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 4. 임베딩(Word2Vec) 변환\n",
    "# ==========================================\n",
    "\n",
    "# 임베딩 설정\n",
    "embedding_dim = 64  # 하나의 단어를 64차원 벡터로 표현\n",
    "\n",
    "# Keras의 Embedding 레이어 객체 생성\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "# 샘플 데이터 하나만 통과시켜보기 (첫 번째 문장)\n",
    "sample_input = goemotions_padded[0]  # (40,) 정수 배열\n",
    "sample_output = embedding_layer(sample_input) # 임베딩 층 통과\n",
    "\n",
    "print(f\"\\n--- [변환 결과 확인] ---\")\n",
    "print(f\"입력 문장(정수 인덱스): {sample_input[:10]} ... (길이: {len(sample_input)})\")\n",
    "print(f\"임베딩 후 벡터 형태(Shape): {sample_output.shape} (문장길이 40 x 벡터차원 64)\")\n",
    "print(f\"첫 번째 단어의 벡터값(일부): {sample_output[0][:5].numpy()} ...\")\n",
    "print(\"\\n------------------------\")\n",
    "print(\"※ 현재 벡터값은 랜덤 초기화 상태입니다. 모델 학습(fit)을 돌리면 의미 있는 값으로 업데이트됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "575eacff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "토크나이저 저장 (나중에 사용하기 위해)\n",
      "================================================================================\n",
      "\n",
      "Tokenizer 저장 완료\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"토크나이저 저장 (나중에 사용하기 위해)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 5. 토크나이저 저장 (나중에 사용하기 위해)\n",
    "# ==========================================\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"\\nTokenizer 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
